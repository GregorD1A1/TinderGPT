from unsloth import FastLanguageModel
import os


current_dir = os.path.dirname(os.path.realpath(__file__))
with open(f'{current_dir}/prompts/writer_tindebielik.prompt', 'r') as file:
    prompt_template = file.read()

parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))
model, tokenizer = FastLanguageModel.from_pretrained(f"{parent_dir}/tindebielik_lora")


def inference_tindebielik(messages, rule):
    prompt = prompt_template.format(tindebielik_rule = rule, messages = messages)
    prompt_chat_template = [{"role": "user", "content": prompt}]
    prompt = tokenizer.apply_chat_template(prompt_chat_template, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=300, use_cache=True, eos_token_id=tokenizer.eos_token_id)
    # returning generated output only
    response = tokenizer.batch_decode(outputs[:, inputs["input_ids"].shape[1]:], skip_special_tokens=True)[0]
    messages = eval(response)
    return messages


if __name__ == "__main__":
    rule = "Kontynuuj flirtowanie."
    messages = """You: A co do koloru... hmmm... Czy to jest fiolet? Nie, moÅ¼e zielony? A moÅ¼e jesteÅ› tajemnicza i nie masz ulubionego koloru? ğŸ¤”
You: No wiÄ™c, opowiedz mi swojÄ… historiÄ™ Å¼ycia, szczegÃ³lnie te zapierajÄ…ce dech w piersiach, a moÅ¼e zaczniemy od ulubionego koloru? ğŸ˜
Girl: Czasem lubiÄ™ irytowaÄ‡ ğŸ˜œ
Girl: A kolor to czarny
Girl: Historia Å¼ycia nudna, ale kiedyÅ› dostaÅ‚am udaru sÅ‚onecznego XD
Girl: Teraz Ty ğŸ˜‚"""

    print(inference_tindebielik(messages, rule))
