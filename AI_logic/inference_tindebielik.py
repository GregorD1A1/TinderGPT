from unsloth import FastLanguageModel
import os


current_dir = os.path.dirname(os.path.realpath(__file__))
with open(f'{current_dir}/prompts/writer_tindebielik.prompt', 'r') as file:
    prompt_template = file.read()

parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))
model, tokenizer = FastLanguageModel.from_pretrained(f"{parent_dir}/tindebielik_lora")


def inference_tindebielik(messages, rule):
    prompt = prompt_template.format(tindebielik_rule = rule, messages = messages)
    prompt_chat_template = [{"role": "user", "content": prompt}]
    prompt = tokenizer.apply_chat_template(prompt_chat_template, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=300, use_cache=True, eos_token_id=tokenizer.eos_token_id)#, temperature=0.5, do_sample=True)
    # returning generated output only
    response = tokenizer.batch_decode(outputs[:, inputs["input_ids"].shape[1]:], skip_special_tokens=True)[0]
    messages = eval(response)
    return messages


if __name__ == "__main__":
    rule = "Kontynuuj flirtowanie."
    messages1 = """You: Cze moja sodka dziewczyno
You: Urocze oczka ma Tw贸j kocio
You: Budujemy raziem hodowl kot贸w i 偶yjemy reszt 偶ycia razem z nasz puchat rodzin?
Girl: Cze Grzegorz!
Girl: A dziki, to kicia o imieniu  Maa
Girl: Hah no niezy pomys 
Girl: Na pewno byo by ciekawie
Girl: Ale Super ta Twoja fota profilowa"""
    messages2 = """You: Hej Nicol, to Ty ratujesz pieski?  Mo偶e by si przydaa mojemu ego - te偶 troch chore... Ale uwaga, jestem bardziej zoliwy ni偶 ka偶dy york na wiecie. き
Girl: Te winki te偶 wygldaj kuszco
Girl: A jeste r贸wnie niezniszczalny co york?
You: Nie, no daj spok贸j, gdzie mi z yorkami si r贸wna
You: Zreszt jakbym chcia sobie nowego yorka kupi, musiabym do Ameryki jecha, co nie?
Girl: Racja
Girl: Dlaczego chore ego?
Girl: Co mu dolega?
"""

    print(inference_tindebielik(messages2, rule))
